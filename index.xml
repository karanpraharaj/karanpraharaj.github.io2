<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Karan Praharaj</title>
    <link>https://karanpraharaj.github.io/</link>
      <atom:link href="https://karanpraharaj.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Karan Praharaj</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© Karan Praharaj, 2020</copyright><lastBuildDate>Sat, 27 Jun 2020 14:24:23 +0530</lastBuildDate>
    <image>
      <url>https://karanpraharaj.github.io/img/sharer.png</url>
      <title>Karan Praharaj</title>
      <link>https://karanpraharaj.github.io/</link>
    </image>
    
    <item>
      <title>Are Algorithms Biased?</title>
      <link>https://karanpraharaj.github.io/post/ai-fairness/</link>
      <pubDate>Sat, 27 Jun 2020 14:24:23 +0530</pubDate>
      <guid>https://karanpraharaj.github.io/post/ai-fairness/</guid>
      <description>&lt;p&gt;After the end of the Second World War, the Nuremberg trials laid bare the atrocities conducted in medical research by the Nazis. In the aftermath of the trials, the medical sciences established a set of rules ‚Äî The Nuremberg Code ‚Äî to control future experiments involving human subjects. The Nuremberg Code has influenced medical codes of ethics around the world, as has the exposure of experiments that had failed to follow it even three decades later, such as the infamous 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Tuskegee_syphilis_experiment&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tuskegee syphilis experiment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The direct negative impact of AI experiments and applications on users isn‚Äôt quite as inhumane as that of the Tuskegee and Nazi experimentations, but in the face of an overwhelming and growing body of evidence of algorithms being biased against certain demographic cohorts, it is important that a dialogue takes place sooner or later. AI systems can be biased based on who builds them, the way they are developed, and how they‚Äôre eventually deployed. This is known as algorithmic bias.&lt;/p&gt;
&lt;p&gt;While the data sciences have not developed a Nuremberg Code of their own yet, the social implications of research in artificial intelligence are starting to be addressed in some curricula. But even as the debates are starting to sprout up, what is still lacking is a discipline-wide discussion to grapple with questions of how to tackle societal and historical inequities that are reinforced by AI algorithms.&lt;/p&gt;
&lt;p&gt;We are flawed creatures. Every single decision we make involves a certain kind of bias. However, algorithms haven‚Äôt proven to be much better. Ideally, we would want our algorithms to make better-informed decisions devoid of biases so as to ensure better social justice, i.e., equal opportunities for individuals and groups (such as minorities) within society to access resources, have their voices heard, and be represented in society.&lt;/p&gt;
&lt;p&gt;When these algorithms do the job of amplifying racial, social and gender inequality, instead of alleviating it; it becomes necessary to take stock of the ethical ramifications and potential malevolence of the technology.&lt;/p&gt;
&lt;p&gt;This essay was motivated by two flashpoints : the racial inequality discussion that is now raging on worldwide, and Yann LeCun‚Äôs altercation with Timnit Gebru on Twitter which was caused due to a disagreement over a downsampled image of Barack Obama (left) that was depixelated to a picture of a white man (right) by a face upsampling machine learning (ML) model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://pbs.twimg.com/media/EbACRFtUYAAjNya?format=jpg&amp;amp;name=large&#34; alt=&#34;Image&#34;&gt;
The (rather explosive) argument was sparked by this tweet by LeCun where he says that the resulting face was that of a white man because of a bias in data that trained the algorithm. Gebru responded sharply that the harms of ML systems cannot be reduced to biased data.&lt;/p&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;ML systems are biased when data is biased.&lt;br&gt;This face upsampling system makes everyone look white because the network was pretrained on FlickFaceHQ, which mainly contains white people pics.&lt;br&gt;Train the *exact* same system on a dataset from Senegal, and everyone will look African. &lt;a href=&#34;https://t.co/jKbPyWYu4N&#34;&gt;https://t.co/jKbPyWYu4N&lt;/a&gt;&lt;/p&gt;&amp;mdash; Yann LeCun (@ylecun) &lt;a href=&#34;https://twitter.com/ylecun/status/1274782757907030016?ref_src=twsrc%5Etfw&#34;&gt;June 21, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;br/&gt;
&lt;p&gt;In most baseline ML algorithms, the model fits better to the attributes or patterns that occur most frequently across various data points. For example, if you were to design an AI recruiting tool to review the r√©sum√©s of applicants for a software engineering position, you would first need to train it with a dataset of past candidates which contains details like ‚Äúexperience‚Äù, ‚Äúqualifications‚Äù, ‚Äúdegree(s) held‚Äù, ‚Äúpast projects‚Äù etc. For every datapoint, the algorithm of the hiring tool would need a decision or a ‚Äúlabel‚Äù, so as to ‚Äúlearn‚Äù how to make a decision for a given applicant by observing patterns in their r√©sum√©.&lt;/p&gt;
&lt;p&gt;For an industry where the gender disparity in representation is large, it is reasonable to assume that a large majority of the data points will be male applicants. And this collective imbalance in the data ends up being interpreted by the algorithm as a useful pattern in the data rather than undesirable noise which is to be ignored. Consequently, it teaches itself that male candidates are more preferable than female candidates.&lt;/p&gt;
&lt;p&gt;I wish that this was merely an imaginary, exaggerated example that I used to prove my point. 
&lt;a href=&#34;https://in.reuters.com/article/amazon-com-jobs-automation/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idINKCN1MK0AH&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;It is not.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LeCun wasn‚Äôt wrong in his assessment because in the case of that specific model, training the model on a dataset that contains faces of black people (as opposed to one that contains mainly white faces) would not have given rise to an output as absurd as that. But the upside of the godfather of modern AI getting dragged into a spat (albeit unfairly) has meant that more researchers will now be aware of the implications of their research.&lt;/p&gt;
&lt;p&gt;The misunderstanding clearly seems to emanate from the interpretation of the word ‚Äúbias‚Äù ‚Äî which in any discussion about the social impact of ML/AI seems to get crushed under the burden of its own weight.&lt;/p&gt;
&lt;p&gt;As Sebastian Raschka puts it, ‚Äúthe term &lt;strong&gt;bias&lt;/strong&gt; in ML is heavily overloaded‚Äù. It has multiple senses that can all be mistaken for each other.&lt;/p&gt;
&lt;p&gt;(1) &lt;strong&gt;bias&lt;/strong&gt; (as in mathematical &lt;strong&gt;bias&lt;/strong&gt; unit)  (2) ‚ÄúFairness‚Äù &lt;strong&gt;bias&lt;/strong&gt; (also called societal &lt;strong&gt;bias&lt;/strong&gt;)  (3) ML &lt;strong&gt;bias&lt;/strong&gt; (also known as inductive &lt;strong&gt;bias&lt;/strong&gt;, which is dependent on decisions taken to build the model.)  (4) &lt;strong&gt;bias&lt;/strong&gt;-variance decomposition of a loss function  (5) Dataset &lt;strong&gt;bias&lt;/strong&gt; (usually causing 2)&lt;/p&gt;
&lt;p&gt;I imagine that a lot of gaps in communication could be covered by just being a little more precise when we use these terms.&lt;/p&gt;
&lt;p&gt;On a lighter note, never mind Obama, the model even depixelized a &lt;strong&gt;dog‚Äôs face&lt;/strong&gt; to a caucasian man‚Äôs. It sure loves white males.&lt;/p&gt;
&lt;br/&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is how that depixelizing algorithm reconstructed my dog, Tank &lt;a href=&#34;https://t.co/XHgdNwRmXy&#34;&gt;pic.twitter.com/XHgdNwRmXy&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jiahao Chen @ üè°üóΩ (@acidflask) &lt;a href=&#34;https://twitter.com/acidflask/status/1274889347356069888?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;br/&gt;
&lt;p&gt;Learning algorithms have inductive biases going beyond the biases in data too, sure. But if the data has a little bias, it is amplified by these systems, thereby causing high biases to be learnt by the model. Simply put, creating a 100% non-biased dataset is practically impossible. Any dataset picked by humans is cherry-picked and non-exhaustive. Our social cognitive biases result in inadvertent cherry-picking of data. This biased data, when fed to a data-variant model (a model whose decisions are heavily influenced by the data it sees) encodes these societal, racial, gender, cultural and political biases and bakes them into the ML model.&lt;/p&gt;
&lt;p&gt;These problems are exacerbated, once they are applied to products. A couple of years ago, Jacky Alcin√© 
&lt;a href=&#34;https://twitter.com/jackyalcine/status/615329515909156865&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pointed out&lt;/a&gt; that the image recognition algorithms in 
&lt;a href=&#34;https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Photos were classifying his black friends as ‚Äúgorillas.‚Äù&lt;/a&gt; Google apologised for the blunder and assured to resolve the issue. However, instead of coming up with a proper solution, it simply blocked the algorithm from identifying gorillas at all.&lt;/p&gt;
&lt;p&gt;It might seem surprising that a company of Google‚Äôs size was unable to come up with a solution to this. But this only goes to show that training an algorithm that is consistent and fair isn‚Äôt an easy proposition, not least when it is not trained and tested on a diverse set of categories that represent various demographic cohorts of the population proportionately.&lt;/p&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Facial recognition is the prime example. Amazon&amp;#39;s Rekognition correctly identifies light-skinned males with an accuracy of 99% but the accuracy drops drastically for females, who are identified as men 19% of the time. It mistakes dark-skinned women for men 39% of the time.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1136048103008690176?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;br/&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.npr.org/2020/06/24/882683463/the-computer-got-it-wrong-how-facial-recognition-led-to-a-false-arrest-in-michig&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Another disastrous episode&lt;/a&gt; of facial recognition tech getting it terribly wrong came as recently as last week when a faulty facial recognition match led to a Michigan man‚Äôs arrest for a crime he did not commit. Recent studies by 
&lt;a href=&#34;https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;M.I.T.&lt;/a&gt; and the 
&lt;a href=&#34;https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Institute of Standards and Technology&lt;/a&gt;, or NIST, found that even though face recognition works well on white men, the results are not good enough for other demographics (the misidentification ratio can be more than 10 times worse), in part because of a lack of diversity in the images used to develop the underlying databases.&lt;/p&gt;
&lt;p&gt;Problems of algorithmic bias are not limited to image/video tasks and they manifest themselves in language tasks too.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language is always ‚Äúsituated‚Äù&lt;/a&gt;, i.e., it depends on external references for its understanding and the receiver(s) must be in a position to resolve these references. This therefore means that the text used to train models carries latent information about the author and the situation, albeit to varying degrees.&lt;/p&gt;
&lt;p&gt;Due to the situatedness of language, any language data set inevitably carries with it a demographic bias. For example, speech to text transcription tends to have higher error rates for African Americans, Arabs and South Asians as compared to Americans and Europeans. Another example in this space is the gender biases in existing word embeddings (which are learned through a neural networks) that show females having a higher association with ‚Äúless-cerebral‚Äù occupations while males tend to be associated with purportedly ‚Äúmore-cerebral‚Äù or higher paying occupations.&lt;/p&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34; data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;For instance - in existing embeddings, it&amp;#39;s observed that women &amp;amp; men are associated with different professions, with men associated with leaderships roles and professions like doctor, programmer and women closer to professions like receptionist or nurse.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1134415294162538497?ref_src=twsrc%5Etfw&#34;&gt;May 31, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;p&gt;In the table below, we see the gender bias scores associated with various occupations in the 
&lt;a href=&#34;https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Universal Sentence Encoder&lt;/a&gt; embedding model. The occupations with positive scores are female-biased occupations and ones with negative scores are male-biased occupations.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;tablefair.png&#34; alt=&#34;fair&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For ML Researchers it would be easy to punt the blame and absolve themselves of all responsibility, but it is imperative for them to acknowledge that they‚Äîknowingly or otherwise‚Äîbuild the base layer of AI products for a lot of companies that are devoid of AI expertise. These companies, without the knowledge of fine-tuning and tweaking models, use pre-trained models, as they are, put out on the internet by ML researchers (like GloVe, BERT, ResNet, YOLO etc).&lt;/p&gt;
&lt;p&gt;Deploying these models without explicitly recalibrating them to account for demographic differences can thus lead to issues of exclusion and overgeneralisation of people along the way. The buck stops with the researchers who must own up responsibility for the other side of the coin.&lt;/p&gt;
&lt;p&gt;It is also easy to blame the data and not the algorithm. (It reminds me of the Republican stance on the second amendment debate : ‚ÄúGuns don‚Äôt kill people, people kill people.‚Äù) Pinning the blame on just the data is irresponsible and akin to saying that the racist child isn&amp;rsquo;t racist because he was taught the racism by his racist father.&lt;/p&gt;
&lt;p&gt;More than we need to improve the data, it is the algorithms that need to be made more robust, less sensitive and less prone to being biased by the data. This needs to be a responsibility for anyone who does research. In the meantime, de-bias the data.&lt;/p&gt;
&lt;p&gt;The guiding question for deployment of algorithms in the real world should always be ‚Äúwould a false answer be worse than no answer?‚Äù&lt;/p&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Thanks to Nayan K, Naga Karthik and Bina Praharaj for reviewing drafts of this.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;[&#34;&gt;Facial Recognition Is Accurate, if You‚Äôre a White Guy&lt;/a&gt;&lt;a href=&#34;https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html&#34;&gt;https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html&lt;/a&gt;) by Steve Lohr&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Krishnapriya, KS., Vangara, K., King, M., Albiero, V., Bowyer, K. 
&lt;a href=&#34;https://arxiv.org/pdf/1904.07325.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Characterizing the Variability in Face Recognition Accuracy Relative to Race&lt;/a&gt; in &lt;em&gt;The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://web.stanford.edu/~mjkay/LifeOfLanguage.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Life of Language&lt;/a&gt; by Martin Kay, Stanford University&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Text Embedding Models Contain Bias. Here&amp;rsquo;s Why That Matters.&lt;/a&gt; by Ben Packer, Yoni Halpern, Mario Guajardo-C√©spedes &amp;amp; Margaret Mitchell, Google AI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bolukbasi, T., Chang, KW., Zou, J., Saligrama, V., Kalai, A. 
&lt;a href=&#34;http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings&lt;/a&gt; in &lt;em&gt;Advances in Neural Information Processing Systems 29, 2016.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>You Are Building Self-Driving Cars. For Free.</title>
      <link>https://karanpraharaj.github.io/post/selfdriving-captcha/</link>
      <pubDate>Fri, 05 Jun 2020 03:03:14 +0530</pubDate>
      <guid>https://karanpraharaj.github.io/post/selfdriving-captcha/</guid>
      <description>&lt;p&gt;Everytime you solve a reCaptcha image, like 
&lt;a href=&#34;https://3.bp.blogspot.com/-zyPGTDdb_1I/W8gUyBlM9QI/AAAAAAAAe7M/Uj0vOL5JTRUoayUosmwswptEDIkgVT28ACLcBGAs/s400/pic.jpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this one&lt;/a&gt;, you are unwittingly cleaning the data which trains machine-learning algorithms deployed in self-driving cars. This twitter thread explains how.&lt;/p&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Wonder how many people realise that they are training algorithms for self-driving cars when they solve captchas&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268613825772716032?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;When you solve a captcha image, you&amp;#39;re now doing more than just proving that you&amp;#39;re not a robot. You are actively teaching a (future) self-driving car to see stuff and identify it.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652639115456512?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This is similar to teaching a baby to identify stuff by showing them the same things over and over again until they learn from their mistakes and start getting their identifications right. This feedback loop of learning-testing-evaluating-relearning is at the heart of this.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652642751954947?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;To be able to self-drive, your AI needs to learn to process, assess and then act on its surroundings. One way to &amp;quot;learn&amp;quot; the &amp;quot;surroundings&amp;quot; is to see a shitload of images of objects that it can potentially encounter in an on-road environment, and learn to classify them correctly.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652644572291072?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Self-driving car companies develop machine learning algorithms that automatically classify these images (one eg. is classifying whether a zebra crossing is present in the field of view or not).&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652646279393280?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;These classifier algos are then fed or &amp;quot;trained&amp;quot; with tens of millions of pictures, until their accuracy crosses a certain minimum threshold. However, merely getting more predictions right than wrong doesn&amp;#39;t cut it for the self-driving problem. There are literal lives at stake.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652648506494983?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Even 80% isn&amp;#39;t good enough. (Imagine misidentifying stuff 20% of the time while driving on road.) The pictures have to be sufficiently exhaustive of possible scenarios AND need the classifier to barely make any mistakes on them.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652650284924929?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;And so because it is crucial to stamp out almost ALL wrong predictions made by these algos, they need to be verified. This is where you - as someone who is much more reliable at identifying stuff than the algo is - come in to the picture.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652652038180864?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;These soft classified images are presented to you in the form of captcha images of streets, traffic lights, pedestrian crossings, animals, footpaths, other vehicles etc.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652653782982656?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;When your captcha-selection agrees with the classifier&amp;#39;s prediction (&amp;quot;annotation&amp;quot;), the confidence in that prediction increases. When there is a mismatch, the confidence drops. If the confidence drops beyond a certain threshold, the prediction is replaced with the &amp;quot;correct&amp;quot; label&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652655502594050?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This task of annotating and labeling billions of images that capture an infinite number of road-situations/-surroundings, is virtually impossible for any company with a limited workforce.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652657264246786?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;So somebody very clever came along and devised this method to trick us all into working for them without paying us a single penny. Now all your successful captchas are used to fine-tune the data that trains the brains of these cars.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652659025817601?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;(This is a very simplistic explanation of self-driving which doesn&amp;#39;t even venture into how the actuations interact with the decision-systems. Self-driving is a much, much harder problem than this thread might make you believe.)&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652660770603011?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;center&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-conversation=&#34;none&#34;  data-theme=&#34;dark&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Next time you see an autonomous vehicle, you know that you made it what it is today.&lt;/p&gt;&amp;mdash; Karan (@IntrepidIndian) &lt;a href=&#34;https://twitter.com/IntrepidIndian/status/1268652662557487108?ref_src=twsrc%5Etfw&#34;&gt;June 4, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/center&gt;
&lt;br/&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;You can follow me on Twitter 
&lt;a href=&#34;https://twitter.com/IntrepidIndian&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Identification Of Lexico-Semantic Word Relations - A Beginner&#39;s Guide</title>
      <link>https://karanpraharaj.github.io/post/lexico-semantic/</link>
      <pubDate>Sun, 24 May 2020 19:07:15 +0530</pubDate>
      <guid>https://karanpraharaj.github.io/post/lexico-semantic/</guid>
      <description>&lt;p&gt;When I ask you to &amp;ldquo;please bear with me&amp;rdquo;, you prepare yourself with the prospect of having to put up with me. If you are told &amp;ldquo;this teddy bear is fluffy&amp;rdquo;, your brain conjures up the image of a soft, lovable, furry toy that toddlers take to bed. If you read about bears being endangered, you think of a polar bear somewhere in the arctic sea ice. And when they will tell you on the news next week that the coronavirus crisis has plunged us into the worst bear market of our generation, you will instinctively know that they are speaking about stocks. Your knowledge of the English language, along with your ability to understand context-dependencies, lexical and syntactic structures, and linguistic nuances helps you differentiate between four senses of the same word. You take it for granted and barely think about it but it has taken you years to acquire this ability. You learn, directly or indirectly, from your personal experiences. You learn through making associations between contexts, information, behaviours, and responses. The cascade of neurocognitive reactions that are set off as you subconsciously trigger a set of neutrons to communicate or listen is nothing short of a wonder. All of this together with your genetic endowment, makes language effortless for you.&lt;/p&gt;
&lt;p&gt;On the other hand, understanding human language is a difficult problem for computers. Unlike you and me, computers do not have the privilege of language training the way we do. Even programming languages aren&amp;rsquo;t directly interpreted by them - they are first converted to low-level machine language. True &lt;em&gt;machine code&lt;/em&gt; is merely a stream of raw, usually binary (1s and 0s), data. While humans acquire the ability to parse, process, infer and communicate, for the computer, any word picked out from a human language is unintelligible gibberish until it is adequately trained to understand the language.&lt;/p&gt;
&lt;p&gt;This task of teaching and empowering machines to understand language just as we do, is called Natural Language Processing or NLP. NLP is a branch of artificial intelligence and it is an umbrella itself for many other subproblems. Daily examples of such problems are search, speech recognition, translation, summarization, question-answering etc. But all of this begs the question - if computers can understand nothing but 1s and 0s, how can they make sense of the complexities of human language?&lt;/p&gt;
&lt;h3 id=&#34;word-vectors---representing-words-in-the-form-of-numbers&#34;&gt;&lt;strong&gt;Word Vectors - Representing words in the form of numbers&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Consider a space where all words in the English language are populated based on their semantic character. This imaginary space is such that words sharing similar descriptions or concepts share similar spacial properties. For instance, the words &amp;ldquo;cat&amp;rdquo; and &amp;ldquo;dog&amp;rdquo; would be in close vicinity of each other because the idea of a cat is very similar to the idea of a dog. Both are quadrupedal, domestic species that make for cute pets. For words that are not similar in meaning but represent the same concept, the positions of the words relative to each other encapsulate the relationship. In the semantic space, the relative position of &amp;ldquo;king&amp;rdquo; to the position of &amp;ldquo;queen&amp;rdquo; would be similar to the relative positions between &amp;ldquo;man&amp;rdquo; and &amp;ldquo;woman&amp;rdquo; or &amp;ldquo;boy&amp;rdquo; and &amp;ldquo;girl&amp;rdquo;, because the defining concept that separates the words in all three cases is the same &amp;ndash; gender.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;wv3.png&#34; alt=&#34;wv3&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the example semantic space below, you can see how the vectors for animals like lion, tiger, cheetah, and elephant are very close together. This is intuitive because they are often discussed in similar contexts; for example, these animals are big, wild and, potentially dangerous ‚Äî indeed, the descriptive word &amp;ldquo;wild&amp;rdquo; maps quite closely to this group of animals.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;wv2.png&#34; style=&#34;zoom: 67%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since words in their purest form cannot be interpreted by computers, we dumb them down by mapping the concepts and ideas that are inherent to the words into a representative set of numbers for each word. These sets of numbers are generated or &amp;ldquo;learned&amp;rdquo; algebraically by &amp;ldquo;neural networks&amp;rdquo; (a type of algorithm) and are called &amp;ldquo;word vectors&amp;rdquo;. These word vectors bear the ability to capture information about semantic relationships and syntactic structures across collections of words. Approaches to generating word vectors build on Firth&amp;rsquo;s (1957) &lt;em&gt;distributional hypothesis&lt;/em&gt; which states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You shall know a word by the company it keeps.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Put differently, &lt;strong&gt;words that share similar contexts tend to have similar meanings&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Word vectors can have any number of dimensions, although the standard number is usually 50, 100 or 300. Each of these dimensions represents a meaning or an abstract concept, the degree of which depends upon the numeric weight of the word on that particular dimension. Here is an example to illustrate this. Consider a lexicon of just ten words (rather than millions) and imagine that our word vectors are three-dimensioned (rather than three hundred).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;wv1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, for better understanding, we are imagining that each dimension captures a clearly defined meaning as opposed to an abstract idea. For example, if you imagine that the third dimension represents the concept of &amp;ldquo;fluffiness&amp;rdquo;, then each word&amp;rsquo;s weight on that dimension represents how closely it relates to that concept. It makes perfect sense for the rabbit to have the highest fluffiness factor at 0.45. This is quite a large simplification of word vectors as the dimensions do not hold such clearly defined meanings in reality, but it is a useful and intuitive way to wrap your head around the concept of word vector dimensions. We will not delve into the mathematical details of how neural networks learn word embeddings, because that would involve a long detour into linear algebra. But now you do know the underlying idea that drives the mathematics.&lt;/p&gt;
&lt;h3 id=&#34;lexical-relation-resolution&#34;&gt;Lexical Relation Resolution&lt;/h3&gt;
&lt;p&gt;My current research work is focused on a problem called lexical relation resolution. A lexical relation is a culturally recognized pattern of association that exists between lexical items (a word, a part of a word, or a chain of words) in a language. For example, the lexical relation between &amp;ldquo;open&amp;rdquo; and &amp;ldquo;close&amp;rdquo; is that of antonymy, whereas &amp;ldquo;close&amp;rdquo; and &amp;ldquo;shut&amp;rdquo; are connected by a synonymy relationship. Other asymmetric lexico-semantic relations include co-hyponymy (e.g. phone ‚Üê‚Üí monitor), hypernymy (e.g. phone ‚Üí speakerphone) or meronymy (e.g. phone ‚Üí mouthpiece), etc&lt;/p&gt;
&lt;p&gt;Recognizing the exact nature of the semantic relation holding between a given pair of words is crucial and forms the basis for all the other NLP applications (question-answering, summarization, speech recognition etc.) that I mentioned above.&lt;/p&gt;
&lt;p&gt;Several methods have been proposed in the past to discriminate between multiple semantic relations that hold between a pair of words. But, this continues to remain a difficult task, especially when it comes to distinguishing between certain relations. (e.g synonymy and hyperonymy).&lt;/p&gt;
&lt;h3 id=&#34;research-work---patches-attention-cuboid&#34;&gt;&lt;strong&gt;Research Work - Patches, Attention, Cuboid&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;To solve this problem, our work proposes to investigate the introduction of related words in the neighbourhood of a particular word and gauge the effect it has on the prediction accuracy of word relations. Our original hypothesis was that if each word is augmented by the word vectors of a fixed number of neighbouring words (or &amp;ldquo;patches&amp;rdquo;), improved performance might be attained.&lt;/p&gt;
&lt;p&gt;Many similarity measures exist to account for the lexical semantic relation that links two words. In our case, we use the cosine similarity measure which has proven to be successful in the past for a variety of semantic relations. To put it in plain speak, cosine similarity is a metric used to determine how similar two entities are. We extend the cosine similarity to patches in a straightforward manner. The similarity between two patches is the set of one-to-one cosine similarity measures between all words in their respective patches.&lt;/p&gt;
&lt;p&gt;The next step consists of transforming a patch into a learning input, because although we can draw and visualise patches in our head, the computer needs it in the form of concrete, numeric data to understand it. The 300-dimensional word vectors of each word are compared to the 300-dimensional word vectors of all the words in the patch and a single similarity score for each comparison. In effect, we take 600 values (300 from each of the two words) and simplify them into one with the help of cosine similarity. If we set the no. of neighbours to be 10, that would give us 10 x 10 comparisons, resulting in 100 similarity values. These values together form the &amp;ldquo;intra-patch similarity table&amp;rdquo;. Given below are a examples of four different patches along with their intra-patch scores.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;patch1.png&#34; alt=&#34;patch1&#34; style=&#34;zoom: 67%;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It was decided that in addition to preserving concept-centrality &lt;em&gt;within&lt;/em&gt; patches, it also makes sense to preserve relation-centrality &lt;em&gt;between&lt;/em&gt; patches. It is important to acknowledge that only certain words in the two patches may be central to the decision of whether two words are in a lexical semantic relation. If two patches share a set of close semantically related words that are central to both concepts, the decision process should intuitively be more reliable.&lt;/p&gt;
&lt;p&gt;However, our initial findings showed that the direct introduction of neighbour words did not lead to improvements. We figured that this was mainly due to a loss of concept-centrality that took place as a result of the change in strategy. If word relations are to be assessed by juxtapositioning two patches instead of two words, the required focus on the original word may be diluted.&lt;/p&gt;
&lt;p&gt;The next logical step was to somehow weigh the word vectors based on their centrality to the concept. To do this, we introduced an &lt;strong&gt;attention mechanism&lt;/strong&gt; based on the PageRank algorithm (which is one of the algorithms used by Google for their web search. PageRank was developed to measure the importance of website pages). We use it to assign a weight of centrality to each of the word neighbours in the patch. The more a word is central in the patch, the higher the score it receives. These scores are then to be used as attention weights to the corresponding word vector representations of the neighbours. The objective of this mechanism is to improve the predictive ability of our system based on the importance score of each word vector in the patch. We found that when deployed in combination with the correct architecture, attention-adjusted patches bolstered our model and gave a significant boost to previous results. I would have to tread into extremely technical territory to explain the specifics of the architectures, so for now we will spare ourselves those details.&lt;/p&gt;
&lt;p&gt;Indeed, average improvements can reach 10.6% for binary classification (to make a decision between two relations) and 8% for multi-class classification (decision between more than two relations) over non-patch baseline approaches. As things stand, we believe that we might get even better results if we construct a cuboid, where the word vectors of two words, instead of being collapsed to a single similarity value, are preserved to a greater extent by only compressing them from 600 dimensions to 300 dimensions. (We do this by taking the dot product between the two vectors. Dot product of two 300-dimensional vectors results in a new 300-dimensional vector). Our results with the cuboid have been promising and have shown an enhanced performance on our previous baselines. However, there are many more tests that the model needs to come through before it can be claimed as an outright upgrade over its predecessors. Our next step is to compare our results with models that attempt to solve the same problem. Whichever way it ends, I will be sure to update this blog on our progress.&lt;/p&gt;
&lt;br/&gt;
&lt;p&gt;As for NLP in general, there is no doubt whatsoever in saying that we are still decades, or at best, years, away from being anywhere close to designing an artificial intelligence that speaks and communicates like us. The amount of data needed today to train a computer well for the simplest of tasks is tremendous. We have neither reached the peak in terms of quality of data representations nor do we have enough computational power to scale models trained on current data representations beyond a particular extent.&lt;/p&gt;
&lt;p&gt;On reflection though, it will never stop being crazy to me that with a little knowledge of mathematics, sufficient computational power and a decent familiarity with a programming language, you can teach a completely inanimate object to understand the language of our species. It is quite surreal when you think about it.&lt;/p&gt;
&lt;p&gt;‚Äã&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;This blog post is based on the work carried out with Nesrine Bannour and Houssam Akhmouch, under the supervision of Prof. Ga√´l Dias.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attention for Machine Translation</title>
      <link>https://karanpraharaj.github.io/post/attention/</link>
      <pubDate>Mon, 08 Apr 2019 13:21:02 +0200</pubDate>
      <guid>https://karanpraharaj.github.io/post/attention/</guid>
      <description>&lt;p&gt;We are not cerebrally hardwired to process loads of information at once. However, what we &lt;em&gt;are&lt;/em&gt; good at, is focusing on a part of the information we&amp;rsquo;re given to make sense of it.  When asked to translate a sentence from one language to another, you process the sentence by picking up individual words as you go, skewering them together into phrases and then mentally assigning corresponding words/phrases in the target language for each part. When a written word is presented to you for translation, it kicks off a series of neurocognitive processes in your brain. Your normal language system in the brain reads the word in your native language, but for translation, this usual tendency must be suppressed so the translated meaning can emerge. A specific region of the brain (known as the &amp;ldquo;caudate nucleus&amp;rdquo;) coordinates this activity, much like an orchestral conductor, to produce stunningly complex behaviours. Essentially, the syntactical, phonological, lexical and semantic aspects of the word or sequence of words are encompassed, assimilated and then contextualized to render the said sequence of words into its equivalent in the target language.&lt;/p&gt;
&lt;p&gt;Translations - be it by human or by machine - are not objective in their character, which is to say that there is no &lt;em&gt;unique&lt;/em&gt; translation for any given sentence. Human translation inherently carries the risk of introducing source-language words, grammar or syntax into the target language rendering. A translation by two humans of a relatively long sentence can rarely ever be the exact same. Despite the variance in the final result, though, what does not change is the broad process by which the translations were arrived at. For any target-language word or phrase that was rendered in the translation, the translator paid attention to some parts of the source sentence more than others.&lt;/p&gt;
&lt;p&gt;This innate quality of humans had not been given to machine algorithms until 2015, when Bahdanau, Cho and Bengio introduced the &amp;ldquo;attention&amp;rdquo; mechanism. This mechanism was proposed to maximise translation performance by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. I concede that this sentence may be hard to digest, but we will make sense of it by breaking it down and paying attention to parts of it one at a time. (Very meta, I know.) The impact of this 2015 paper was profound and it would go on to become the building block for several state-of-the-art models.&lt;/p&gt;
&lt;img src=&#34;att1.png&#34;/&gt; 
&lt;h4 id=&#34;why-do-we-need-attention&#34;&gt;Why do we need Attention?&lt;/h4&gt;
&lt;p&gt;In conventional neural machine translation models, an encoder-decoder combination is used with an encoder and a decoder for each language, or a language-specific encoder applied to each sentence whose outputs are then compared. An encoder RNN reads and encodes a source sentence into a fixed-length vector.  A decoder then spits out a translation based on the vector fed to it from the encoder. The whole encoder‚Äìdecoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.&lt;/p&gt;
&lt;img src=&#34;att2.png&#34; alt=&#34;frog&#34;/&gt; 
&lt;h5 id=&#34;the-bottleneck&#34;&gt;The Bottleneck&lt;/h5&gt;
&lt;p&gt;A problem with this architecture is its over-reliance on one fixed-length vector to encompass all the necessary information and be a good quality representation of the source sentence. This pressure on the fixed-vector vector to compress and capture all the information is a bottleneck and it makes it difficult for the encoder neural network to perform well on long sentences. It has been shown earlier that the performance of a basic encoder‚Äìdecoder deteriorates rapidly as the length of an input sentence increases.&lt;/p&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;h4 id=&#34;core-idea&#34;&gt;Core Idea&lt;/h4&gt;
&lt;p&gt;In order to address the issue, Bahdanau et al. introduced an extension to the encoder-decoder model which learns to align and translate jointly. The new architecture deploys a bidirectional-RNN as an encoder and a decoder that will be able to focus on all hidden states instead of just the final hidden state. What this modification does, is afford the decoder a flexibility in decision-making and therefore identify the parts of the source sentence that may be more relevant for the prediction of the next word in the target sentence. This is the intuition of the attention mechanism, and it leads us now to the mathematics that goes into making this happen.&lt;/p&gt;
&lt;h4 id=&#34;the-algebra-involved&#34;&gt;The Algebra Involved&lt;/h4&gt;
&lt;p&gt;In the basic encoder-decoder RNN framework, the decoder is trained to predict the next word $y_t$ , given the context vector $c$ and all the words that have been predicted in previous time steps {$y_1$,‚Ä¶,$y_{t^{&#39;}-1}$,$c$}&lt;/p&gt;
&lt;p&gt;In the new model architecture however, the probability is conditioned on a distinct context vector $c_i$ for each target word $y_t$.&lt;/p&gt;
&lt;p&gt;The probability over translation $y$ is defined as :
$$p(\textbf{y}) = \prod_{t=1}^{T} p(y_t|{y_1,&amp;hellip;,y_{t-1}},c_i)\tag{1}$$
where $\textbf{y}$ is the output (predicted translation). The condition probability in (1) is defined as :
$$p(y_i| {y_1,&amp;hellip;,y_{t-1}}, \textbf{x})) = g(y_{i-1},s_i,c_i) \tag{2}$$&lt;/p&gt;
&lt;p&gt;where $\textbf{x}$ is the source sentence and $s_i$ is an RNN hidden state for timestep $i$ determined by $s_i = f(s_{i-1},y_{i-1},c_i)$ .&lt;/p&gt;
&lt;p&gt;The source sentence is mapped by the encoder to a sequence of annotations, $h_1,‚Ä¶h_{T_x}$, whose weighted sum is computed to obtain the context vector $c_i$. Each annotation encapsulates information about the entire input sequence with a strong focus on the parts surround the $i^{th}$ word of the input sequence.&lt;/p&gt;
&lt;p&gt;The encoder is a bidirectional-RNN, and so the annotations for each word $x_j$ are obtained by concatenating the forward hidden state $\vec{h}_j$ along with the backward one $\overleftarrow{h}_j$ , i.e. $h_j = \bigg[\vec{h_j^T} ;\overleftarrow{h_j^T}\bigg]$. This representation of inputs helps contain important bits of information from words in the neighbourhood of $x_j$.&lt;/p&gt;
&lt;img src=&#34;att3.png&#34; alt=&#34;frog&#34;/&gt; 
&lt;p&gt;Now that we have established the idea of annotations, we can proceed to discuss the computation of the context vectors $c_i$. The context vector is computed as the weight sum of annotations :
$$ c_i = \sum_{j=1}^{T_x}\alpha_{ij}h_{j}\tag{3} $$
The weight $\alpha$ corresponding to each annotation $h_j$ is calculated by taking  softmax over the attention scores:
$$ \alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})}, \tag{4} $$
where
$$ e_{ij} = a(s_{i-1},h_{j}) $$
is a scoring model which quantifies how strong the inputs around position $j$ and output at position $i$ align. The alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly.&lt;/p&gt;
&lt;p&gt;Thus, this new approach facilitates the information to be spread across the sequence of annotations, which can be selectively retrieved by the decoder accordingly.&lt;/p&gt;
&lt;p&gt;By medium of language, we manage to communicate ideas over long ranges of space and time, but the creation of syntactic bonds between words in a sentence that may or may not be in close proximity to each other, underpins expression of ideas in any language. This is where attention steps in and aids the mapping of syntaxes from the source language to the target language. To identify relationships of words with other words that maybe far away in the same sentence ‚Äî all while ignoring other words that just do not have much influence on the word we&amp;rsquo;re trying to predict ‚Äî that is what attention aims to do.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;¬†&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;[1]	Bahdanau, D., Cho, K. &amp; Bengio, Y. &lt;a href = http://arxiv.org/abs/1409.0473&gt;Neural machine translation by jointly learning to align and translate. &lt;/a&gt; In &lt;i&gt;Proc. International Conference on Learning Representations&lt;/i&gt; (2015)&lt;/div&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;[2]	Cho, Kyunghyun, Aaron Courville, and Yoshua Bengio. &lt;a href = http://arxiv.org/abs/1507.01053&gt;Describing Multimedia Content using Attention-based Encoder‚ÄìDecoder Networks.&lt;/a&gt;  &amp;nbsp  (2015)&lt;/div&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;[3]	Sutskever, I. Vinyals, O. &amp; Le. Q. V. &lt;a href =  https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&gt; Sequence to sequence learning with neural networks.&lt;/a&gt; In &lt;i&gt;Proc. Advances in Neural Information Processing Systems.&lt;/i&gt; (2014)&lt;/div&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;[4]	Chris Olah&#39;s blog post. &lt;a href = https://distill.pub/2016/augmented-rnns/&gt;&#34;Attention and Augmented Recurrent Neural Networks&#34; &lt;/div&gt; 
</description>
    </item>
    
  </channel>
</rss>
