[{"authors":["Karan Praharaj"],"categories":null,"content":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof. Ga√´l Dias, focuses on Patch-based Identification of Lexico-Semantic Relations. If that is too much jargon, here is a dummy\u0026rsquo;s summary which anyone can read.\nMy research interests include Lexical Semantics and Deep Learning models for the prediction of symmetrical and asymmetrical relations between words.\nWhen time isn\u0026rsquo;t tight, I like to read and learn about International Relations, public policy and macroeconomics.\nIf I\u0026rsquo;m working, I\u0026rsquo;m probably also listening to music. And if I\u0026rsquo;m listening to music, it\u0026rsquo;s probably one of these playlists on my Spotify.\nMy twitter feed is a lot livelier than this blog. You can follow me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"718e013125a163e108bb79fe0e207104","permalink":"https://karanpraharaj.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof.","tags":null,"title":"","type":"authors"},{"authors":["Karan Praharaj"],"categories":["Computer Science"],"content":"After the end of the war, the Nuremberg trials laid bare the atrocities conducted in medical research by the Nazis. In the aftermath of the trials, the medical sciences established a set of rules ‚Äî The Nuremberg Code ‚Äî to control future experiments involving human subjects. The Nuremberg Code has influenced medical codes of ethics around the world, as has the exposure of experiments that had failed to follow it even three decades later, such as the infamous Tuskegee syphilis experiment.\nThe direct impact of AI experiments and applications on users isn\u0026rsquo;t quite as inhumane as the Tuskegee and Nazi experimentations, but in the face of an overwhelming and growing body of evidence of algorithms being biased against certain demographic cohorts, it is important that the discussion happens sooner or later. AI systems can be biased based on who builds them, the way they are developed, and how they‚Äôre eventually deployed. This is known as algorithmic bias.\nWhile the data sciences have not developed a Nuremberg Code of their own yet, the social implications of research in artificial intelligence are starting to be addressed in some curricula. But even as the debates are starting to sprout up, what is still lacking is a discipline-wide discussion to grapple with questions of how to tackle societal and historical inequities that are reinforced by AI algorithms.\nWe are flawed creatures. Every single decision we make involves a certain kind of bias. However, algorithms haven\u0026rsquo;t proven to be much better. Ideally, we would want our algorithms to make better-informed decisions devoid of biases so as to ensure better social justice, i.e., equal opportunities for individuals and groups (such as minorities) within society to access resources, have their voices heard, and be represented in society.\nWhen these algorithms do the job of amplifying racial, social and gender inequality, instead of alleviating it; it becomes necessary to take stock of the ethical ramifications and potential malevolence of the technology.\nThis essay was motivated by two flashpoints : the racial inequality discussion that is now raging on worldwide, and the Yann LeCun v/s Timnit Gebru beef on Twitter caused due to a disagreement over a downsampled image of Barack Obama which was depixelated to a picture of a white man by a face upsampling machine learning (ML) model.\nThe (rather explosive) argument was sparked by this tweet by LeCun where he says that the resulting face was that of a white man because of a bias in data that trained the algorithm. Gebru responded sharply that the harms of ML systems cannot be reduced to biased data.\nNever mind Obama, the model even depixelized a dog\u0026rsquo;s face to a caucasian man\u0026rsquo;s. It sure loves the white man.\n This is how that depixelizing algorithm reconstructed my dog, Tank pic.twitter.com/XHgdNwRmXy\n\u0026mdash; Jiahao Chen @ üè°üóΩ (@acidflask) June 22, 2020   The misunderstanding clearly seems to emanate over the interpretation of the word \u0026ldquo;bias\u0026rdquo; - which in any discussion about the social impact of ML/AI seems to get crushed under the burden of its own weight.\nAs Sebastian Raschka puts it, \u0026ldquo;the term bias in ML is heavily overloaded\u0026rdquo;. It has multiple senses that can all be mistaken for each other and a lot of gaps in communication could be covered by just being a little more precise.\n‚Äã\t(1) bias (as in mathematical bias unit) ‚Äã\t(2) \u0026ldquo;Fairness\u0026rdquo; bias (also called societal bias) ‚Äã\t(3) ML bias (also known as inductive bias, which is dependent on decisions taken to build the model.) ‚Äã\t(4) bias-variance decomposition of a loss function\n‚Äã\t(5) Dataset bias (usually causing 2)\n ML systems are biased when data is biased.\nThis face upsampling system makes everyone look white because the network was pretrained on FlickFaceHQ, which mainly contains white people pics.\nTrain the *exact* same system on a dataset from Senegal, and everyone will look African. https://t.co/jKbPyWYu4N\n\u0026mdash; Yann LeCun (@ylecun) June 21, 2020   LeCun wasn\u0026rsquo;t wrong because in the case of that specific model, training the model on a dataset that contains faces of black people (as opposed to one that contains mainly white faces) would not have given rise to an output as absurd as that. But the upside of the godfather of modern AI getting dragged into a spat (albeit unfairly) has meant that more researchers will now be aware of the implications of their research.\nLearning algorithms have inductive biases going beyond the biases in data too, sure. But if the data has a little bias, it is amplified by these systems, thereby causing high biases to be learnt by the model. Simply put, creating a 100% non-biased dataset is practically impossible. Any dataset picked by humans is cherry-picked and non-exhaustive. Our social cognitive biases result in inadvertent cherry-picking of data. This biased data, when fed to a data-variant model (a model whose decisions are heavily influenced by the data it sees) encodes these societal, racial, gender, cultural and politicial biases and bakes them into the ML model.\nThese problems exacerbate, once they are applied to products. A couple of years ago, Jacky Alcin√© pointed out that the image recognition algorithms in Google Photos were classifying his black friends as ‚Äúgorillas.‚Äù Google apologised for the blunder and assured to resolve the issue. However, instead of coming up with a proper solution, it simply blocked the algorithm from identifying gorillas at all.\nIt might seem surprising that a company of Google\u0026rsquo;s size was unable to come up with a solution to this. But this only goes to show how that training an algorithm to be consistent and fair isn\u0026rsquo;t an easy proposition, not least when it is not trained and tested on a diverse set of categories.\nFacial recognition is the prime example. Amazon\u0026#39;s Rekognition correctly identifies light-skinned males with an accuracy of 99% but the accuracy drops drastically for females, who are identified as men 19% of the time. It mistakes dark-skinned women for men 39% of the time.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2019    Another disastrous episode of facial recognition tech getting it terribly wrong came as recently as last week when a faulty facial recognition match led to a Michigan man‚Äôs arrest for a crime he did not commit. Recent studies by M.I.T. and the National Institute of Standards and Technology, or NIST, found that even though face recognition works well on white men, the results are not good enough for other demographics, in part because of a lack of diversity in the images used to develop the underlying databases.\nProblems of algorithmic bias are not limited to image/video tasks and they manifest themselves in language tasks too.\n Language is always \u0026ldquo;situated\u0026rdquo;, i.e., it depends on external references for its understanding and the receiver(s) must be in a position to resolve these references. This therefore means that the text used to train models carries latent information about the author and the situation, albeit to varying degrees.\nDue to the situatedness of language, any language data set inevitably carries with it a demographic bias. For example, speech to text transcription tends to have higher error rates for African Americans, Arabs and South Asians as compared to Americans and Europeans. Another example in this space is the gender biases in existing word embeddings (which are learned through a neural networks) that show females having a higher association with \u0026ldquo;less-cerebral\u0026rdquo; occupations while males tend to be associated with traditionally \u0026ldquo;more-cerebral\u0026rdquo; or higher paying occupations.\nFor instance - in existing embeddings, it\u0026#39;s observed that women \u0026amp; men are associated with different professions, with men associated with leaderships roles and professions like doctor, programmer and women closer to professions like receptionist or nurse.\n\u0026mdash; Karan (@IntrepidIndian) May 31, 2019  While it is easy for ML Researchers to hold their hands up and absolve themselves of all responsibility, they are preparing the foundations of products of a lot of companies that are devoid of AI expertise. These companies, without the knowledge of fine-tuning and tweaking, use pre-trained models put out on the internet by ML researchers (like GloVe, BERT, ResNet, YOLO etc). Deploying these models without explicitly recalibrating them to account for demographic differences can thus lead to issues of exclusion and overgeneralisation of people along the way. The buck stops with the researchers who must own up responsibility for the other side of the coin.\nIt is also easy to blame the data and not the algorithm. (It reminds me of the Republican stance on the second amendment debate : \u0026ldquo;Guns don\u0026rsquo;t kill people, people kill people.\u0026quot;) But what is indisputable is that more than the need to improve the data, it is the algorithms that need to be more robust and less prone to being biased by the data. In the meantime, de-bias the data.\n¬† The guiding question for deployment of algorithms in the real world should always be ‚Äúwould a false answer be worse than no answer?‚Äù\n¬†  ¬† References ¬†   Facial Recognition Is Accurate, if You‚Äôre a White Guyhttps://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html) by Steve Lohr\n  Krishnapriya, KS., Vangara, K., King, M., Albiero, V., Bowyer, K. Characterizing the Variability in Face Recognition Accuracy Relative to Race in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.\n   Life of Language by Martin Kay, Stanford University\n   Text Embedding Models Contain Bias. Here\u0026rsquo;s Why That Matters. by Ben Packer, Yoni Halpern, Mario Guajardo-C√©spedes \u0026amp; Margaret Mitchell, Google AI\n  Bolukbasi, T., Chang, KW., Zou, J., Saligrama, V., Kalai, A. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings in Advances in Neural Information Processing Systems 29, 2016.\n  ¬† ","date":1593248063,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593248063,"objectID":"729a4c71ea46545163ba31c24effbc47","permalink":"https://karanpraharaj.github.io/post/ai-fairness/","publishdate":"2020-06-27T14:24:23+05:30","relpermalink":"/post/ai-fairness/","section":"post","summary":"Algorithms do what they're taught. Unfortunately some are inadvertently taught prejudices and unethical biases by societal patterns hidden in the  data.","tags":["Artificial Intelligence","AI Fairness"],"title":"How Are Algorithms Biased?","type":"post"},{"authors":["Karan Praharaj"],"categories":["Computer Science","Twitter Threads"],"content":"Everytime you solve a reCaptcha image, like this one, you are unwittingly cleaning the data which trains machine-learning algorithms deployed in self-driving cars. This twitter thread explains how.\nWonder how many people realise that they are training algorithms for self-driving cars when they solve captchas\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  When you solve a captcha image, you\u0026#39;re now doing more than just proving that you\u0026#39;re not a robot. You are actively teaching a (future) self-driving car to see stuff and identify it.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  This is similar to teaching a baby to identify stuff by showing them the same things over and over again until they learn from their mistakes and start getting their identifications right. This feedback loop of learning-testing-evaluating-relearning is at the heart of this.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  To be able to self-drive, your AI needs to learn to process, assess and then act on its surroundings. One way to \u0026quot;learn\u0026quot; the \u0026quot;surroundings\u0026quot; is to see a shitload of images of objects that it can potentially encounter in an on-road environment, and learn to classify them correctly.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  Self-driving car companies develop machine learning algorithms that automatically classify these images (one eg. is classifying whether a zebra crossing is present in the field of view or not).\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  These classifier algos are then fed or \u0026quot;trained\u0026quot; with tens of millions of pictures, until their accuracy crosses a certain minimum threshold. However, merely getting more predictions right than wrong doesn\u0026#39;t cut it for the self-driving problem. There are literal lives at stake.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  Even 80% isn\u0026#39;t good enough. (Imagine misidentifying stuff 20% of the time while driving on road.) The pictures have to be sufficiently exhaustive of possible scenarios AND need the classifier to barely make any mistakes on them.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  And so because it is crucial to stamp out almost ALL wrong predictions made by these algos, they need to be verified. This is where you - as someone who is much more reliable at identifying stuff than the algo is - come in to the picture.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  These soft classified images are presented to you in the form of captcha images of streets, traffic lights, pedestrian crossings, animals, footpaths, other vehicles etc.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  When your captcha-selection agrees with the classifier\u0026#39;s prediction (\u0026quot;annotation\u0026quot;), the confidence in that prediction increases. When there is a mismatch, the confidence drops. If the confidence drops beyond a certain threshold, the prediction is replaced with the \u0026quot;correct\u0026quot; label\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  This task of annotating and labeling billions of images that capture an infinite number of road-situations/-surroundings, is virtually impossible for any company with a limited workforce.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  So somebody very clever came along and devised this method to trick us all into working for them without paying us a single penny. Now all your successful captchas are used to fine-tune the data that trains the brains of these cars.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  (This is a very simplistic explanation of self-driving which doesn\u0026#39;t even venture into how the actuations interact with the decision-systems. Self-driving is a much, much harder problem than this thread might make you believe.)\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020  Next time you see an autonomous vehicle, you know that you made it what it is today.\n\u0026mdash; Karan (@IntrepidIndian) June 4, 2020    You can follow me on Twitter here.\n","date":1591306394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591306394,"objectID":"05334962970aaeaeae974f1d4f2e4081","permalink":"https://karanpraharaj.github.io/post/selfdriving-captcha/","publishdate":"2020-06-05T03:03:14+05:30","relpermalink":"/post/selfdriving-captcha/","section":"post","summary":"We are all unwittingly cleaning the data which trains algorithms deployed in self-driving cars.","tags":["Deep Learning","Artificial Intelligence"],"title":"You Are Building Self-Driving Cars. For Free.","type":"post"},{"authors":["Karan Praharaj"],"categories":["Computer Science"],"content":"When I ask you to \u0026ldquo;please bear with me\u0026rdquo;, you prepare yourself with the prospect of having to put up with me. If you are told \u0026ldquo;this teddy bear is fluffy\u0026rdquo;, your brain conjures up the image of a soft, lovable, furry toy that toddlers take to bed. If you read about bears being endangered, you think of a polar bear somewhere in the arctic sea ice. And when they will tell you on the news next week that the coronavirus crisis has plunged us into the worst bear market of our generation, you will instinctively know that they are speaking about stocks. Your knowledge of the English language, along with your ability to understand context-dependencies, lexical and syntactic structures, and linguistic nuances helps you differentiate between four senses of the same word. You take it for granted and barely think about it but it has taken you years to acquire this ability. You learn, directly or indirectly, from your personal experiences. You learn through making associations between contexts, information, behaviours, and responses. The cascade of neurocognitive reactions that are set off as you subconsciously trigger a set of neutrons to communicate or listen is nothing short of a wonder. All of this together with your genetic endowment, makes language effortless for you.\nOn the other hand, understanding human language is a difficult problem for computers. Unlike you and me, computers do not have the privilege of language training the way we do. Even programming languages aren\u0026rsquo;t directly interpreted by them - they are first converted to low-level machine language. True machine code is merely a stream of raw, usually binary (1s and 0s), data. While humans acquire the ability to parse, process, infer and communicate, for the computer, any word picked out from a human language is unintelligible gibberish until it is adequately trained to understand the language.\nThis task of teaching and empowering machines to understand language just as we do, is called Natural Language Processing or NLP. NLP is a branch of artificial intelligence and it is an umbrella itself for many other subproblems. Daily examples of such problems are search, speech recognition, translation, summarization, question-answering etc. But all of this begs the question - if computers can understand nothing but 1s and 0s, how can they make sense of the complexities of human language?\nWord Vectors - Representing words in the form of numbers Consider a space where all words in the English language are populated based on their semantic character. This imaginary space is such that words sharing similar descriptions or concepts share similar spacial properties. For instance, the words \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;dog\u0026rdquo; would be in close vicinity of each other because the idea of a cat is very similar to the idea of a dog. Both are quadrupedal, domestic species that make for cute pets. For words that are not similar in meaning but represent the same concept, the positions of the words relative to each other encapsulate the relationship. In the semantic space, the relative position of \u0026ldquo;king\u0026rdquo; to the position of \u0026ldquo;queen\u0026rdquo; would be similar to the relative positions between \u0026ldquo;man\u0026rdquo; and \u0026ldquo;woman\u0026rdquo; or \u0026ldquo;boy\u0026rdquo; and \u0026ldquo;girl\u0026rdquo;, because the defining concept that separates the words in all three cases is the same \u0026ndash; gender.\nIn the example semantic space below, you can see how the vectors for animals like lion, tiger, cheetah, and elephant are very close together. This is intuitive because they are often discussed in similar contexts; for example, these animals are big, wild and, potentially dangerous ‚Äî indeed, the descriptive word \u0026ldquo;wild\u0026rdquo; maps quite closely to this group of animals.\nSince words in their purest form cannot be interpreted by computers, we dumb them down by mapping the concepts and ideas that are inherent to the words into a representative set of numbers for each word. These sets of numbers are generated or \u0026ldquo;learned\u0026rdquo; algebraically by \u0026ldquo;neural networks\u0026rdquo; (a type of algorithm) and are called \u0026ldquo;word vectors\u0026rdquo;. These word vectors bear the ability to capture information about semantic relationships and syntactic structures across collections of words. Approaches to generating word vectors build on Firth\u0026rsquo;s (1957) distributional hypothesis which states:\n \u0026ldquo;You shall know a word by the company it keeps.\u0026rdquo;\n Put differently, words that share similar contexts tend to have similar meanings.\nWord vectors can have any number of dimensions, although the standard number is usually 50, 100 or 300. Each of these dimensions represents a meaning or an abstract concept, the degree of which depends upon the numeric weight of the word on that particular dimension. Here is an example to illustrate this. Consider a lexicon of just ten words (rather than millions) and imagine that our word vectors are three-dimensioned (rather than three hundred).\nIn the figure above, for better understanding, we are imagining that each dimension captures a clearly defined meaning as opposed to an abstract idea. For example, if you imagine that the third dimension represents the concept of \u0026ldquo;fluffiness\u0026rdquo;, then each word\u0026rsquo;s weight on that dimension represents how closely it relates to that concept. It makes perfect sense for the rabbit to have the highest fluffiness factor at 0.45. This is quite a large simplification of word vectors as the dimensions do not hold such clearly defined meanings in reality, but it is a useful and intuitive way to wrap your head around the concept of word vector dimensions. We will not delve into the mathematical details of how neural networks learn word embeddings, because that would involve a long detour into linear algebra. But now you do know the underlying idea that drives the mathematics.\nLexical Relation Resolution My current research work is focused on a problem called lexical relation resolution. A lexical relation is a culturally recognized pattern of association that exists between lexical items (a word, a part of a word, or a chain of words) in a language. For example, the lexical relation between \u0026ldquo;open\u0026rdquo; and \u0026ldquo;close\u0026rdquo; is that of antonymy, whereas \u0026ldquo;close\u0026rdquo; and \u0026ldquo;shut\u0026rdquo; are connected by a synonymy relationship. Other asymmetric lexico-semantic relations include co-hyponymy (e.g. phone ‚Üê‚Üí monitor), hypernymy (e.g. phone ‚Üí speakerphone) or meronymy (e.g. phone ‚Üí mouthpiece), etc\nRecognizing the exact nature of the semantic relation holding between a given pair of words is crucial and forms the basis for all the other NLP applications (question-answering, summarization, speech recognition etc.) that I mentioned above.\nSeveral methods have been proposed in the past to discriminate between multiple semantic relations that hold between a pair of words. But, this continues to remain a difficult task, especially when it comes to distinguishing between certain relations. (e.g synonymy and hyperonymy).\nResearch Work - Patches, Attention, Cuboid To solve this problem, our work proposes to investigate the introduction of related words in the neighbourhood of a particular word and gauge the effect it has on the prediction accuracy of word relations. Our original hypothesis was that if each word is augmented by the word vectors of a fixed number of neighbouring words (or \u0026ldquo;patches\u0026rdquo;), improved performance might be attained.\nMany similarity measures exist to account for the lexical semantic relation that links two words. In our case, we use the cosine similarity measure which has proven to be successful in the past for a variety of semantic relations. To put it in plain speak, cosine similarity is a metric used to determine how similar two entities are. We extend the cosine similarity to patches in a straightforward manner. The similarity between two patches is the set of one-to-one cosine similarity measures between all words in their respective patches.\nThe next step consists of transforming a patch into a learning input, because although we can draw and visualise patches in our head, the computer needs it in the form of concrete, numeric data to understand it. The 300-dimensional word vectors of each word are compared to the 300-dimensional word vectors of all the words in the patch and a single similarity score for each comparison. In effect, we take 600 values (300 from each of the two words) and simplify them into one with the help of cosine similarity. If we set the no. of neighbours to be 10, that would give us 10 x 10 comparisons, resulting in 100 similarity values. These values together form the \u0026ldquo;intra-patch similarity table\u0026rdquo;. Given below are a examples of four different patches along with their intra-patch scores.\nIt was decided that in addition to preserving concept-centrality within patches, it also makes sense to preserve relation-centrality between patches. It is important to acknowledge that only certain words in the two patches may be central to the decision of whether two words are in a lexical semantic relation. If two patches share a set of close semantically related words that are central to both concepts, the decision process should intuitively be more reliable.\nHowever, our initial findings showed that the direct introduction of neighbour words did not lead to improvements. We figured that this was mainly due to a loss of concept-centrality that took place as a result of the change in strategy. If word relations are to be assessed by juxtapositioning two patches instead of two words, the required focus on the original word may be diluted.\nThe next logical step was to somehow weigh the word vectors based on their centrality to the concept. To do this, we introduced an attention mechanism based on the PageRank algorithm (which is one of the algorithms used by Google for their web search. PageRank was developed to measure the importance of website pages). We use it to assign a weight of centrality to each of the word neighbours in the patch. The more a word is central in the patch, the higher the score it receives. These scores are then to be used as attention weights to the corresponding word vector representations of the neighbours. The objective of this mechanism is to improve the predictive ability of our system based on the importance score of each word vector in the patch. We found that when deployed in combination with the correct architecture, attention-adjusted patches bolstered our model and gave a significant boost to previous results. I would have to tread into extremely technical territory to explain the specifics of the architectures, so for now we will spare ourselves those details.\nIndeed, average improvements can reach 10.6% for binary classification (to make a decision between two relations) and 8% for multi-class classification (decision between more than two relations) over non-patch baseline approaches. As things stand, we believe that we might get even better results if we construct a cuboid, where the word vectors of two words, instead of being collapsed to a single similarity value, are preserved to a greater extent by only compressing them from 600 dimensions to 300 dimensions. (We do this by taking the dot product between the two vectors. Dot product of two 300-dimensional vectors results in a new 300-dimensional vector). Our results with the cuboid have been promising and have shown an enhanced performance on our previous baselines. However, there are many more tests that the model needs to come through before it can be claimed as an outright upgrade over its predecessors. Our next step is to compare our results with models that attempt to solve the same problem. Whichever way it ends, I will be sure to update this blog on our progress.\n As for NLP in general, there is no doubt whatsoever in saying that we are still decades, or at best, years, away from being anywhere close to designing an artificial intelligence that speaks and communicates like us. The amount of data needed today to train a computer well for the simplest of tasks is tremendous. We have neither reached the peak in terms of quality of data representations nor do we have enough computational power to scale models trained on current data representations beyond a particular extent.\nOn reflection though, it will never stop being crazy to me that with a little knowledge of mathematics, sufficient computational power and a decent familiarity with a programming language, you can teach a completely inanimate object to understand the language of our species. It is quite surreal when you think about it.\n‚Äã\n This blog post is based on the work carried out with Nesrine Bannour and Houssam Akhmouch, under the supervision of Prof. Ga√´l Dias.\n","date":1590327435,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590327435,"objectID":"a65b0da3b3bde3890a283e7c58a3e5f7","permalink":"https://karanpraharaj.github.io/post/lexico-semantic/","publishdate":"2020-05-24T19:07:15+05:30","relpermalink":"/post/lexico-semantic/","section":"post","summary":"A dummy's guide that explains what I have been upto these last few months.","tags":["Deep Learning","Natural Language Processing","Word Similarity"],"title":"Identification Of Lexico-Semantic Word Relations - A Beginner's Guide","type":"post"},{"authors":["Karan Praharaj"],"categories":["Computer Science"],"content":"We are not cerebrally hardwired to process loads of information at once. However, what we are good at, is focusing on a part of the information we\u0026rsquo;re given to make sense of it. When asked to translate a sentence from one language to another, you process the sentence by picking up individual words as you go, skewering them together into phrases and then mentally assigning corresponding words/phrases in the target language for each part. When a written word is presented to you for translation, it kicks off a series of neurocognitive processes in your brain. Your normal language system in the brain reads the word in your native language, but for translation, this usual tendency must be suppressed so the translated meaning can emerge. A specific region of the brain (known as the \u0026ldquo;caudate nucleus\u0026rdquo;) coordinates this activity, much like an orchestral conductor, to produce stunningly complex behaviours. Essentially, the syntactical, phonological, lexical and semantic aspects of the word or sequence of words are encompassed, assimilated and then contextualized to render the said sequence of words into its equivalent in the target language.\nTranslations - be it by human or by machine - are not objective in their character, which is to say that there is no unique translation for any given sentence. Human translation inherently carries the risk of introducing source-language words, grammar or syntax into the target language rendering. A translation by two humans of a relatively long sentence can rarely ever be the exact same. Despite the variance in the final result, though, what does not change is the broad process by which the translations were arrived at. For any target-language word or phrase that was rendered in the translation, the translator paid attention to some parts of the source sentence more than others.\nThis innate quality of humans had not been given to machine algorithms until 2015, when Bahdanau, Cho and Bengio introduced the \u0026ldquo;attention\u0026rdquo; mechanism. This mechanism was proposed to maximise translation performance by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. I concede that this sentence may be hard to digest, but we will make sense of it by breaking it down and paying attention to parts of it one at a time. (Very meta, I know.) The impact of this 2015 paper was profound and it would go on to become the building block for several state-of-the-art models.\nWhy do we need Attention? In conventional neural machine translation models, an encoder-decoder combination is used with an encoder and a decoder for each language, or a language-specific encoder applied to each sentence whose outputs are then compared. An encoder RNN reads and encodes a source sentence into a fixed-length vector. A decoder then spits out a translation based on the vector fed to it from the encoder. The whole encoder‚Äìdecoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.\nThe Bottleneck A problem with this architecture is its over-reliance on one fixed-length vector to encompass all the necessary information and be a good quality representation of the source sentence. This pressure on the fixed-vector vector to compress and capture all the information is a bottleneck and it makes it difficult for the encoder neural network to perform well on long sentences. It has been shown earlier that the performance of a basic encoder‚Äìdecoder deteriorates rapidly as the length of an input sentence increases.\n¬†Core Idea In order to address the issue, Bahdanau et al. introduced an extension to the encoder-decoder model which learns to align and translate jointly. The new architecture deploys a bidirectional-RNN as an encoder and a decoder that will be able to focus on all hidden states instead of just the final hidden state. What this modification does, is afford the decoder a flexibility in decision-making and therefore identify the parts of the source sentence that may be more relevant for the prediction of the next word in the target sentence. This is the intuition of the attention mechanism, and it leads us now to the mathematics that goes into making this happen.\nThe Algebra Involved In the basic encoder-decoder RNN framework, the decoder is trained to predict the next word $y_t$ , given the context vector $c$ and all the words that have been predicted in previous time steps {$y_1$,‚Ä¶,$y_{t^{'}-1}$,$c$}\nIn the new model architecture however, the probability is conditioned on a distinct context vector $c_i$ for each target word $y_t$.\nThe probability over translation $y$ is defined as : $$p(\\textbf{y}) = \\prod_{t=1}^{T} p(y_t|{y_1,\u0026hellip;,y_{t-1}},c_i)\\tag{1}$$ where $\\textbf{y}$ is the output (predicted translation). The condition probability in (1) is defined as : $$p(y_i| {y_1,\u0026hellip;,y_{t-1}}, \\textbf{x})) = g(y_{i-1},s_i,c_i) \\tag{2}$$\nwhere $\\textbf{x}$ is the source sentence and $s_i$ is an RNN hidden state for timestep $i$ determined by $s_i = f(s_{i-1},y_{i-1},c_i)$ .\nThe source sentence is mapped by the encoder to a sequence of annotations, $h_1,‚Ä¶h_{T_x}$, whose weighted sum is computed to obtain the context vector $c_i$. Each annotation encapsulates information about the entire input sequence with a strong focus on the parts surround the $i^{th}$ word of the input sequence.\nThe encoder is a bidirectional-RNN, and so the annotations for each word $x_j$ are obtained by concatenating the forward hidden state $\\vec{h}_j$ along with the backward one $\\overleftarrow{h}_j$ , i.e. $h_j = \\bigg[\\vec{h_j^T} ;\\overleftarrow{h_j^T}\\bigg]$. This representation of inputs helps contain important bits of information from words in the neighbourhood of $x_j$.\nNow that we have established the idea of annotations, we can proceed to discuss the computation of the context vectors $c_i$. The context vector is computed as the weight sum of annotations : $$ c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_{j}\\tag{3} $$ The weight $\\alpha$ corresponding to each annotation $h_j$ is calculated by taking softmax over the attention scores: $$ \\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}, \\tag{4} $$ where $$ e_{ij} = a(s_{i-1},h_{j}) $$ is a scoring model which quantifies how strong the inputs around position $j$ and output at position $i$ align. The alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly.\nThus, this new approach facilitates the information to be spread across the sequence of annotations, which can be selectively retrieved by the decoder accordingly.\nBy medium of language, we manage to communicate ideas over long ranges of space and time, but the creation of syntactic bonds between words in a sentence that may or may not be in close proximity to each other, underpins expression of ideas in any language. This is where attention steps in and aids the mapping of syntaxes from the source language to the target language. To identify relationships of words with other words that maybe far away in the same sentence ‚Äî all while ignoring other words that just do not have much influence on the word we\u0026rsquo;re trying to predict ‚Äî that is what attention aims to do.\n ¬†References [1]\tBahdanau, D., Cho, K. \u0026 Bengio, Y. Neural machine translation by jointly learning to align and translate.  In Proc. International Conference on Learning Representations (2015) [2]\tCho, Kyunghyun, Aaron Courville, and Yoshua Bengio. Describing Multimedia Content using Attention-based Encoder‚ÄìDecoder Networks. \u0026nbsp (2015) [3]\tSutskever, I. Vinyals, O. \u0026 Le. Q. V. Sequence to sequence learning with neural networks. In Proc. Advances in Neural Information Processing Systems. (2014) [4]\tChris Olah's blog post. \"Attention and Augmented Recurrent Neural Networks\"  ","date":1554722462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554722462,"objectID":"2e16cbd1e0682a2cea47acb2ee507f80","permalink":"https://karanpraharaj.github.io/post/attention/","publishdate":"2019-04-08T13:21:02+02:00","relpermalink":"/post/attention/","section":"post","summary":"A brief round-up of the attention mechanism for neural machine translation models.","tags":["Deep Learning","Natural Language Processing"],"title":"Attention for Machine Translation","type":"post"}]