[{"authors":["Karan Praharaj"],"categories":null,"content":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof. Gaël Dias, focuses on Patch-based Identification of Lexico-Semantic Relations. If that is too much jargon, here is a dummy\u0026rsquo;s summary which anyone can read.\nMy research interests include Lexical Semantics and Deep Learning models for the prediction of symmetrical and asymmetrical relations between words.\nWhen time isn\u0026rsquo;t tight, I like to read and learn about International Relations, public policy and macroeconomics.\nIf I\u0026rsquo;m working, I\u0026rsquo;m probably also listening to music. And if I\u0026rsquo;m listening to music, it\u0026rsquo;s probably one of these playlists on my Spotify.\nMy twitter feed is a lot livelier than this blog. You can follow me here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"718e013125a163e108bb79fe0e207104","permalink":"https://karanpraharaj.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof.","tags":null,"title":"","type":"authors"},{"authors":["Karan Praharaj"],"categories":[],"content":"When I tell you \u0026ldquo;this teddy bear is fluffy\u0026rdquo;,\nUnderstanding human language is a difficult problem for computers. Unlike you and me, computers do not have the privilege of human language training the way we do. Even programming languages aren\u0026rsquo;t directly interpreted by them - they are first converted to low-level machine language. True machine code is a stream of raw, usually binary (1s and 0s), data.\nWhile humans acquire the ability to parse, process, infer and communicate \u0026ndash; all of which we are doing right now, by way of this essay \u0026ndash; for the computer, any word picked out from a human language is unintelligible gibberish until it is adequately trained to understand the language.\nThis task of teaching and empowering machines to understand language just as we do, is called Natural Language Processing or NLP. NLP is a branch of artificial intelligence and it is an umbrella itself for many other subproblems. Daily examples of such problems are search, speech recognition, translation, summarization, question-answering etc. But all of this begs the question - if computers can understand nothing but 1s and 0s, how can computers make sense of the complexities of human language?\nConsider a space where all words in the English language are populated based on their semantic character. This imaginary space is such that words sharing similar share similar spacial properties. For instance, the words \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;dog\u0026rdquo; would be in close vicinity with each other because the idea of a cat is very similar to the idea of a dog. Both are bipedal, domestic species that make for cute pets. For words that are not similar in meaning but represent the same concept, the positions of the words relative to each other encapsulate the relationship. In the semantic space, the relative position of \u0026ldquo;king\u0026rdquo; to the position of \u0026ldquo;queen\u0026rdquo; would be similar to the difference in relative positions between \u0026ldquo;man\u0026rdquo; and \u0026ldquo;woman\u0026rdquo; or \u0026ldquo;boy\u0026rdquo; and \u0026ldquo;girl\u0026rdquo;, because the defining concept that separates the words is the same \u0026ndash; gender.\nSince words in their purest form cannot be interpreted by computers, we dumb them down by mapping the concepts and ideas that are inherent to the words into a representative set of numbers for each word. These sets of numbers are generated or \u0026ldquo;learned\u0026rdquo; algebraically by \u0026ldquo;neural networks\u0026rdquo; (a type of algorithm) and are called \u0026ldquo;word vectors\u0026rdquo;. These word vectors bear the ability to capture information about semantic relationships and syntactic structures across collections of words. Approaches to generating word vectors build on Firth\u0026rsquo;s (1957) distributional hypothesis which states:\n \u0026ldquo;You shall know a word by the company it keeps.\u0026rdquo;\n Put differently, words that share similar contexts tend to have similar meanings.\nWe will not delve into the mathematical details of how neural networks learn word embeddings, but now you know the underlying idea that drives the mathematics.\nMy current research work is focused on a problem called lexical relation resolution. A lexical relation is a culturally recognized pattern of association that exists between lexical items (a word, a part of a word, or a chain of words) in a language. For example, the lexical relation between \u0026ldquo;open\u0026rdquo; and \u0026ldquo;close\u0026rdquo; is that of antonymy, whereas \u0026ldquo;close\u0026rdquo; and \u0026ldquo;shut\u0026rdquo; are connected by a synonymy relationship. Other asymmetric lexico-semantic relations include co-hyponymy (e.g. phone ←→ monitor), hypernymy (e.g. phone → speakerphone) or meronymy (e.g. phone → mouthpiece), etc\nRecognizing the exact nature of the semantic relation holding between a given pair of words is crucial and forms the basis for all the other NLP applications (question-answering, summarization, speech recognition etc.) that I mentioned above.\nSeveral methods have been proposed in the past to discriminate between multiple semantic relations that hold between a pair of words. But, this continues to remain a difficult task, especially when it comes to distinguishing between certain relations. (e.g synonymy and hyperonymy).\nTo solve this problem, our work proposes to investigate the introduction of related words in the neighbourhood of a particular word and gauge the effect it has on the prediction accuracy of word relations. Our original hypothesis was that if each word is augmented by the word vectors of a fixed number of neighbouring words (or \u0026ldquo;patches\u0026rdquo;), improved performance might be attained.\nHowever, our initial findings showed that the direct introduction of neighbour words did not lead to improvements. We figured that this was mainly due to a loss of concept-centrality that took place as a result of the change in strategy. If word relations are to be assessed by juxtapositioning two patches instead of two words, the required focus on the original word may be diluted.\nThe next logical step was to somehow weigh the word vectors based on their centrality to the concept. To do this, we introduced an attention mechanism based on the PageRank algorithm (which is one of the algorithms used by Google for their web search. PageRank was developed to measure the importance of website pages). We use it to assign a weight of centrality to each of the word neighbours in the patch. The more a word is central in the patch, the higher the score it receives. These scores are then to be used as attention weights to the corresponding word vector representations of the neighbours. The objective of this mechanism is to improve the predictive ability of our system based on the importance score of each word embedding in the patch. We found that when deployed in combination with the correct architecture, attention-adjusted patches gave a significant boost to previous results.\n","date":1582378635,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582378635,"objectID":"a65b0da3b3bde3890a283e7c58a3e5f7","permalink":"https://karanpraharaj.github.io/post/lexico-semantic/","publishdate":"2020-02-22T19:07:15+05:30","relpermalink":"/post/lexico-semantic/","section":"post","summary":"When I tell you \u0026ldquo;this teddy bear is fluffy\u0026rdquo;,\nUnderstanding human language is a difficult problem for computers. Unlike you and me, computers do not have the privilege of human language training the way we do.","tags":["Deep Learning","Natural Language Processing","Word Similarity"],"title":"Identification Of Lexico-semantic Word Relations - A Beginner's Guide","type":"post"},{"authors":["Karan Praharaj"],"categories":[],"content":"We are not cerebrally hardwired to process loads of information at once. However, what we are good at, is focusing on a part of the information we\u0026rsquo;re given to make sense of it. When asked to translate a sentence from one language to another, you process the sentence by picking up individual words as you go, skewering them together into phrases and then mentally assigning corresponding words/phrases in the target language for each part. When a written word is presented to you for translation, it kicks off a series of neurocognitive processes in your brain. Your normal language system in the brain reads the word in your native language, but for translation, this usual tendency must be suppressed so the translated meaning can emerge. A specific region of the brain (known as the \u0026ldquo;caudate nucleus\u0026rdquo;) coordinates this activity, much like an orchestral conductor, to produce stunningly complex behaviours. Essentially, the syntactical, phonological, lexical and semantic aspects of the word or sequence of words are encompassed, assimilated and then contextualized to render the said sequence of words into its equivalent in the target language.\nTranslations - be it by human or by machine - are not objective in their character, which is to say that there is no unique translation for any given sentence. Human translation inherently carries the risk of introducing source-language words, grammar or syntax into the target language rendering. A translation by two humans of a relatively long sentence can rarely ever be the exact same. Despite the variance in the final result, though, what does not change is the broad process by which the translations were arrived at. For any target-language word or phrase that was rendered in the translation, the translator paid attention to some parts of the source sentence more than others.\nThis innate quality of humans had not been given to machine algorithms until 2015, when Bahdanau, Cho and Bengio introduced the \u0026ldquo;attention\u0026rdquo; mechanism. This mechanism was proposed to maximise translation performance by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. I concede that this sentence may be hard to digest, but we will make sense of it by breaking it down and paying attention to parts of it one at a time. (Very meta, I know.) The impact of this 2015 paper was profound and it would go on to become the building block for several state-of-the-art models.\nWhy do we need Attention? In conventional neural machine translation models, an encoder-decoder combination is used with an encoder and a decoder for each language, or a language-specific encoder applied to each sentence whose outputs are then compared. An encoder RNN reads and encodes a source sentence into a fixed-length vector. A decoder then spits out a translation based on the vector fed to it from the encoder. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.\nThe Bottleneck A problem with this architecture is its over-reliance on one fixed-length vector to encompass all the necessary information and be a good quality representation of the source sentence. This pressure on the fixed-vector vector to compress and capture all the information is a bottleneck and it makes it difficult for the encoder neural network to perform well on long sentences. It has been shown earlier that the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.\n Core Idea In order to address the issue, Bahdanau et al. introduced an extension to the encoder-decoder model which learns to align and translate jointly. The new architecture deploys a bidirectional-RNN as an encoder and a decoder that will be able to focus on all hidden states instead of just the final hidden state. What this modification does, is afford the decoder a flexibility in decision-making and therefore identify the parts of the source sentence that may be more relevant for the prediction of the next word in the target sentence. This is the intuition of the attention mechanism, and it leads us now to the mathematics that goes into making this happen.\nThe Algebra Involved In the basic encoder-decoder RNN framework, the decoder is trained to predict the next word $y_t$ , given the context vector $c$ and all the words that have been predicted in previous time steps {$y_1$,…,$y_{t^{'}-1}$,$c$}\nIn the new model architecture however, the probability is conditioned on a distinct context vector $c_i$ for each target word $y_t$.\nThe probability over translation $y$ is defined as : $$p(\\textbf{y}) = \\prod_{t=1}^{T} p(y_t|{y_1,\u0026hellip;,y_{t-1}},c_i)\\tag{1}$$ where $\\textbf{y}$ is the output (predicted translation). The condition probability in (1) is defined as : $$p(y_i| {y_1,\u0026hellip;,y_{t-1}}, \\textbf{x})) = g(y_{i-1},s_i,c_i) \\tag{2}$$\nwhere $\\textbf{x}$ is the source sentence and $s_i$ is an RNN hidden state for timestep $i$ determined by $s_i = f(s_{i-1},y_{i-1},c_i)$ .\nThe source sentence is mapped by the encoder to a sequence of annotations, $h_1,…h_{T_x}$, whose weighted sum is computed to obtain the context vector $c_i$. Each annotation encapsulates information about the entire input sequence with a strong focus on the parts surround the $i^{th}$ word of the input sequence.\nThe encoder is a bidirectional-RNN, and so the annotations for each word $x_j$ are obtained by concatenating the forward hidden state $\\vec{h}_j$ along with the backward one $\\overleftarrow{h}_j$ , i.e. $h_j = \\bigg[\\vec{h_j^T} ;\\overleftarrow{h_j^T}\\bigg]$. This representation of inputs helps contain important bits of information from words in the neighbourhood of $x_j$.\nNow that we have established the idea of annotations, we can proceed to discuss the computation of the context vectors $c_i$. The context vector is computed as the weight sum of annotations : $$ c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_{j}\\tag{3} $$ The weight $\\alpha$ corresponding to each annotation $h_j$ is calculated by taking softmax over the attention scores: $$ \\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}, \\tag{4} $$ where $$ e_{ij} = a(s_{i-1},h_{j}) $$ is a scoring model which quantifies how strong the inputs around position $j$ and output at position $i$ align. The alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly.\nThus, this new approach facilitates the information to be spread across the sequence of annotations, which can be selectively retrieved by the decoder accordingly.\nBy medium of language, we manage to communicate ideas over long ranges of space and time, but the creation of syntactic bonds between words in a sentence that may or may not be in close proximity to each other, underpins expression of ideas in any language. This is where attention steps in and aids the mapping of syntaxes from the source language to the target language. To identify relationships of words with other words that maybe far away in the same sentence — all while ignoring other words that just do not have much influence on the word we\u0026rsquo;re trying to predict — that is what attention aims to do.\n  References [1]\tBahdanau, D., Cho, K. \u0026 Bengio, Y. Neural machine translation by jointly learning to align and translate.  In Proc. International Conference on Learning Representations (2015) [2]\tCho, Kyunghyun, Aaron Courville, and Yoshua Bengio. Describing Multimedia Content using Attention-based Encoder–Decoder Networks. \u0026nbsp (2015) [3]\tSutskever, I. Vinyals, O. \u0026 Le. Q. V. Sequence to sequence learning with neural networks. In Proc. Advances in Neural Information Processing Systems. (2014) [4]\tChris Olah's blog post. \"Attention and Augmented Recurrent Neural Networks\"  ","date":1554722462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554722462,"objectID":"2e16cbd1e0682a2cea47acb2ee507f80","permalink":"https://karanpraharaj.github.io/post/attention/","publishdate":"2019-04-08T13:21:02+02:00","relpermalink":"/post/attention/","section":"post","summary":"A brief round-up of the attention mechanism for neural machine translation models.","tags":["Deep Learning","Natural Language Processing"],"title":"Attention for Machine Translation","type":"post"}]