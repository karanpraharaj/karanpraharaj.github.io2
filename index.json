[{"authors":["admin"],"categories":null,"content":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof. Gaël Dias, focuses on Patch-based Identification of Lexico-Semantic Relations. If that is too much jargon, here is a dummy\u0026rsquo;s summary which anyone can read.\nMy research interests include Lexical Semantics and Deep Learning models for the prediction of symmetrical and asymmetrical relations between words.\nWhen time isn\u0026rsquo;t tight, I like to read and learn about International Relations, public policy and macroeconomics.\nIf I\u0026rsquo;m working, I\u0026rsquo;m probably also listening to music. And if I\u0026rsquo;m listening to music, it\u0026rsquo;s probably one of these playlists on my Spotify.\nMy twitter feed is a lot livelier than this blog. You can follow me here.\n","date":1582378635,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582378635,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://karanpraharaj.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a student working in Computational Linguistics and Natural Language Processing. Currently, I\u0026rsquo;m visiting the GREYC Lab @ University of Caen, Normandy in France, as a research assistant. My work here, supervised by Prof.","tags":null,"title":"","type":"authors"},{"authors":[""],"categories":[],"content":"When I tell you that the \u0026ldquo;teddy bear factory has\u0026rdquo;\n","date":1582378635,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582378635,"objectID":"a65b0da3b3bde3890a283e7c58a3e5f7","permalink":"https://karanpraharaj.github.io/post/lexico-semantic/","publishdate":"2020-02-22T19:07:15+05:30","relpermalink":"/post/lexico-semantic/","section":"post","summary":"When I tell you that the \u0026ldquo;teddy bear factory has\u0026rdquo;","tags":["Deep Learning","Natural Language Processing","Word Similarity"],"title":"Identification Of Lexico-semantic Word Relations - A Beginner's Guide","type":"post"},{"authors":["Karan Praharaj"],"categories":[],"content":"We are not cerebrally hardwired to process loads of information at once. However, what we are good at, is focusing on a part of the information we\u0026rsquo;re given to make sense of it. When asked to translate a sentence from one language to another, you process the sentence by picking up individual words as you go, skewering them together into phrases and then mentally assigning corresponding words/phrases in the target language for each part. When a written word is presented to you for translation, it kicks off a series of neurocognitive processes in your brain. Your normal language system in the brain reads the word in your native language, but for translation, this usual tendency must be suppressed so the translated meaning can emerge. A specific region of the brain (known as the \u0026ldquo;caudate nucleus\u0026rdquo;) coordinates this activity, much like an orchestral conductor, to produce stunningly complex behaviours. Essentially, the syntactical, phonological, lexical and semantic aspects of the word or sequence of words are encompassed, assimilated and then contextualized to render the said sequence of words into its equivalent in the target language.\nTranslations - be it by human or by machine - are not objective in their character, which is to say that there is no unique translation for any given sentence. Human translation inherently carries the risk of introducing source-language words, grammar or syntax into the target language rendering. A translation by two humans of a relatively long sentence can rarely ever be the exact same. Despite the variance in the final result, though, what does not change is the broad process by which the translations were arrived at. For any target-language word or phrase that was rendered in the translation, the translator paid attention to some parts of the source sentence more than others.\nThis innate quality of humans had not been given to machine algorithms until 2015, when Bahdanau, Cho and Bengio introduced the \u0026ldquo;attention\u0026rdquo; mechanism. This mechanism was proposed to maximise translation performance by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. I concede that this sentence may be hard to digest, but we will make sense of it by breaking it down and paying attention to parts of it one at a time. (Very meta, I know.) The impact of this 2015 paper was profound and it would go on to become the building block for several state-of-the-art models.\nWhy do we need Attention? In conventional neural machine translation models, an encoder-decoder combination is used with an encoder and a decoder for each language, or a language-specific encoder applied to each sentence whose outputs are then compared. An encoder RNN reads and encodes a source sentence into a fixed-length vector. A decoder then spits out a translation based on the vector fed to it from the encoder. The whole encoder–decoder system, which consists of the encoder and the decoder for a language pair, is jointly trained to maximize the probability of a correct translation given a source sentence.\nThe Bottleneck A problem with this architecture is its over-reliance on one fixed-length vector to encompass all the necessary information and be a good quality representation of the source sentence. This pressure on the fixed-vector vector to compress and capture all the information is a bottleneck and it makes it difficult for the encoder neural network to perform well on long sentences. It has been shown earlier that the performance of a basic encoder–decoder deteriorates rapidly as the length of an input sentence increases.\n Core Idea In order to address the issue, Bahdanau et al. introduced an extension to the encoder-decoder model which learns to align and translate jointly. The new architecture deploys a bidirectional-RNN as an encoder and a decoder that will be able to focus on all hidden states instead of just the final hidden state. What this modification does, is afford the decoder a flexibility in decision-making and therefore identify the parts of the source sentence that may be more relevant for the prediction of the next word in the target sentence. This is the intuition of the attention mechanism, and it leads us now to the mathematics that goes into making this happen.\nThe Algebra Involved In the basic encoder-decoder RNN framework, the decoder is trained to predict the next word $y_t$ , given the context vector $c$ and all the words that have been predicted in previous time steps {$y_1$,…,$y_{t^{'}-1}$,$c$}\nIn the new model architecture however, the probability is conditioned on a distinct context vector $c_i$ for each target word $y_t$.\nThe probability over translation $y$ is defined as : $$p(\\textbf{y}) = \\prod_{t=1}^{T} p(y_t|{y_1,\u0026hellip;,y_{t-1}},c_i)\\tag{1}$$ where $\\textbf{y}$ is the output (predicted translation). The condition probability in (1) is defined as : $$p(y_i| {y_1,\u0026hellip;,y_{t-1}}, \\textbf{x})) = g(y_{i-1},s_i,c_i) \\tag{2}$$\nwhere $\\textbf{x}$ is the source sentence and $s_i$ is an RNN hidden state for timestep $i$ determined by $s_i = f(s_{i-1},y_{i-1},c_i)$ .\nThe source sentence is mapped by the encoder to a sequence of annotations, $h_1,…h_{T_x}$, whose weighted sum is computed to obtain the context vector $c_i$. Each annotation encapsulates information about the entire input sequence with a strong focus on the parts surround the $i^{th}$ word of the input sequence.\nThe encoder is a bidirectional-RNN, and so the annotations for each word $x_j$ are obtained by concatenating the forward hidden state $\\vec{h}_j$ along with the backward one $\\overleftarrow{h}_j$ , i.e. $h_j = \\bigg[\\vec{h_j^T} ;\\overleftarrow{h_j^T}\\bigg]$. This representation of inputs helps contain important bits of information from words in the neighbourhood of $x_j$.\nNow that we have established the idea of annotations, we can proceed to discuss the computation of the context vectors $c_i$. The context vector is computed as the weight sum of annotations : $$ c_i = \\sum_{j=1}^{T_x}\\alpha_{ij}h_{j}\\tag{3} $$ The weight $\\alpha$ corresponding to each annotation $h_j$ is calculated by taking softmax over the attention scores: $$ \\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}, \\tag{4} $$ where $$ e_{ij} = a(s_{i-1},h_{j}) $$ is a scoring model which quantifies how strong the inputs around position $j$ and output at position $i$ align. The alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through. This gradient can be used to train the alignment model as well as the whole translation model jointly.\nThus, this new approach facilitates the information to be spread across the sequence of annotations, which can be selectively retrieved by the decoder accordingly.\nBy medium of language, we manage to communicate ideas over long ranges of space and time, but the creation of syntactic bonds between words in a sentence that may or may not be in close proximity to each other, underpins expression of ideas in any language. This is where attention steps in and aids the mapping of syntaxes from the source language to the target language. To identify relationships of words with other words that maybe far away in the same sentence — all while ignoring other words that just do not have much influence on the word we\u0026rsquo;re trying to predict — that is what attention aims to do.\n  References [1]\tBahdanau, D., Cho, K. \u0026 Bengio, Y. Neural machine translation by jointly learning to align and translate.  In Proc. International Conference on Learning Representations (2015) [2]\tCho, Kyunghyun, Aaron Courville, and Yoshua Bengio. Describing Multimedia Content using Attention-based Encoder–Decoder Networks. \u0026nbsp (2015) [3]\tSutskever, I. Vinyals, O. \u0026 Le. Q. V. Sequence to sequence learning with neural networks. In Proc. Advances in Neural Information Processing Systems. (2014) [4]\tChris Olah's blog post. \"Attention and Augmented Recurrent Neural Networks\"  ","date":1554722462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554722462,"objectID":"2e16cbd1e0682a2cea47acb2ee507f80","permalink":"https://karanpraharaj.github.io/post/attention/","publishdate":"2019-04-08T13:21:02+02:00","relpermalink":"/post/attention/","section":"post","summary":"A brief round-up of the attention mechanism for neural machine translation models.","tags":["Deep Learning","Natural Language Processing"],"title":"Attention for Machine Translation","type":"post"}]