<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word Similarity | Karan Praharaj</title>
    <link>https://karanpraharaj.github.io/tag/word-similarity/</link>
      <atom:link href="https://karanpraharaj.github.io/tag/word-similarity/index.xml" rel="self" type="application/rss+xml" />
    <description>Word Similarity</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 22 Feb 2020 19:07:15 +0530</lastBuildDate>
    <image>
      <url>https://karanpraharaj.github.io/img/sharer.png</url>
      <title>Word Similarity</title>
      <link>https://karanpraharaj.github.io/tag/word-similarity/</link>
    </image>
    
    <item>
      <title>Identification Of Lexico-semantic Word Relations - A Beginner&#39;s Guide</title>
      <link>https://karanpraharaj.github.io/post/lexico-semantic/</link>
      <pubDate>Sat, 22 Feb 2020 19:07:15 +0530</pubDate>
      <guid>https://karanpraharaj.github.io/post/lexico-semantic/</guid>
      <description>&lt;p&gt;When I tell you &amp;ldquo;this teddy bear is fluffy&amp;rdquo;,&lt;/p&gt;
&lt;p&gt;Understanding human language is a difficult problem for computers. Unlike you and me, computers do not have the privilege of human language training the way we do. Even programming languages aren&amp;rsquo;t directly interpreted by them - they are first converted to low-level machine language. True &lt;em&gt;machine code&lt;/em&gt; is a stream of raw, usually binary (1s and 0s), data.&lt;/p&gt;
&lt;p&gt;While humans acquire the ability to parse, process, infer and communicate &amp;ndash; all of which we are doing right now, by way of this essay &amp;ndash; for the computer, any word picked out from a human language is unintelligible gibberish until it is adequately trained to understand the language.&lt;/p&gt;
&lt;p&gt;This task of teaching and empowering machines to understand language just as we do, is called Natural Language Processing or NLP. NLP is a branch of artificial intelligence and it is an umbrella itself for many other subproblems. Daily examples of such problems are search, speech recognition, translation, summarization, question-answering etc. But all of this begs the question - if computers can understand nothing but 1s and 0s, how can computers make sense of the complexities of human language?&lt;/p&gt;
&lt;p&gt;Consider a space where all words in the English language are populated based on their semantic character. This imaginary space is such that words sharing similar share similar spacial properties. For instance, the words &amp;ldquo;cat&amp;rdquo; and &amp;ldquo;dog&amp;rdquo; would be in close vicinity with each other because the idea of a cat is very similar to the idea of a dog. Both are bipedal, domestic species that make for cute pets. For words that are not similar in meaning but represent the same concept, the positions of the words relative to each other encapsulate the relationship. In the semantic space, the relative position of &amp;ldquo;king&amp;rdquo; to the position of &amp;ldquo;queen&amp;rdquo; would be similar to the difference in relative positions between &amp;ldquo;man&amp;rdquo; and &amp;ldquo;woman&amp;rdquo; or &amp;ldquo;boy&amp;rdquo; and &amp;ldquo;girl&amp;rdquo;, because the defining concept that separates the words is the same &amp;ndash; gender.&lt;/p&gt;
&lt;p&gt;Since words in their purest form cannot be interpreted by computers, we dumb them down by mapping the concepts and ideas that are inherent to the words into a representative set of numbers for each word. These sets of numbers are generated or &amp;ldquo;learned&amp;rdquo; algebraically by &amp;ldquo;neural networks&amp;rdquo; (a type of algorithm) and are called &amp;ldquo;word vectors&amp;rdquo;. These word vectors bear the ability to capture information about semantic relationships and syntactic structures across collections of words. Approaches to generating word vectors build on Firth&amp;rsquo;s (1957) &lt;em&gt;distributional hypothesis&lt;/em&gt; which states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;You shall know a word by the company it keeps.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Put differently, &lt;strong&gt;words that share similar contexts tend to have similar meanings&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We will not delve into the mathematical details of how neural networks learn word embeddings, but now you know the underlying idea that drives the mathematics.&lt;/p&gt;
&lt;p&gt;My current research work is focused on a problem called lexical relation resolution. A lexical relation is a culturally recognized pattern of association that exists between lexical items (a word, a part of a word, or a chain of words) in a language. For example, the lexical relation between &amp;ldquo;open&amp;rdquo; and &amp;ldquo;close&amp;rdquo; is that of antonymy, whereas &amp;ldquo;close&amp;rdquo; and &amp;ldquo;shut&amp;rdquo; are connected by a synonymy relationship. Other asymmetric lexico-semantic relations include co-hyponymy (e.g. phone ←→ monitor), hypernymy (e.g. phone → speakerphone) or meronymy (e.g. phone → mouthpiece), etc&lt;/p&gt;
&lt;p&gt;Recognizing the exact nature of the semantic relation holding between a given pair of words is crucial and forms the basis for all the other NLP applications (question-answering, summarization, speech recognition etc.) that I mentioned above.&lt;/p&gt;
&lt;p&gt;Several methods have been proposed in the past to discriminate between multiple semantic relations that hold between a pair of words. But, this continues to remain a difficult task, especially when it comes to distinguishing between certain relations. (e.g synonymy and hyperonymy).&lt;/p&gt;
&lt;p&gt;To solve this problem, our work proposes to investigate the introduction of related words in the neighbourhood of a particular word and gauge the effect it has on the prediction accuracy of word relations. Our original hypothesis was that if each word is augmented by the word vectors of a fixed number of neighbouring words (or &amp;ldquo;patches&amp;rdquo;), improved performance might be attained.&lt;/p&gt;
&lt;p&gt;However, our initial findings showed that the direct introduction of neighbour words did not lead to improvements. We figured that this was mainly due to a loss of concept-centrality that took place as a result of the change in strategy. If word relations are to be assessed by juxtapositioning two patches instead of two words, the required focus on the original word may be diluted.&lt;/p&gt;
&lt;p&gt;The next logical step was to somehow weigh the word vectors based on their centrality to the concept. To do this, we introduced an &lt;strong&gt;attention mechanism&lt;/strong&gt; based on the PageRank algorithm (which is one of the algorithms used by Google for their web search. PageRank was developed to measure the importance of website pages). We use it to assign a weight of centrality to each of the word neighbours in the patch. The more a word is central in the patch, the higher the score it receives. These scores are then to be used as attention weights to the corresponding word vector representations of the neighbours. The objective of this mechanism is to improve the predictive ability of our system based on the importance score of each word embedding in the patch. We found that when deployed in combination with the correct architecture, attention-adjusted patches gave a significant boost to previous results.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
